{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pytorch-transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:07:46.777786Z","iopub.execute_input":"2021-06-28T23:07:46.778146Z","iopub.status.idle":"2021-06-28T23:07:52.629713Z","shell.execute_reply.started":"2021-06-28T23:07:46.778113Z","shell.execute_reply":"2021-06-28T23:07:52.628917Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-transformers in /opt/conda/lib/python3.7/site-packages (1.2.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (0.1.95)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (1.17.40)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (2.25.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (0.0.43)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (2020.11.13)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (1.7.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-transformers) (4.56.2)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.0->pytorch-transformers) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.0->pytorch-transformers) (0.6)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-transformers) (0.3.6)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-transformers) (0.10.0)\nRequirement already satisfied: botocore<1.21.0,>=1.20.40 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-transformers) (1.20.40)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.40->boto3->pytorch-transformers) (2.8.1)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.40->boto3->pytorch-transformers) (1.26.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.40->boto3->pytorch-transformers) (1.15.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-transformers) (2020.12.5)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->pytorch-transformers) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->pytorch-transformers) (1.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:07:52.632559Z","iopub.execute_input":"2021-06-28T23:07:52.632867Z","iopub.status.idle":"2021-06-28T23:07:54.552766Z","shell.execute_reply.started":"2021-06-28T23:07:52.632837Z","shell.execute_reply":"2021-06-28T23:07:54.552014Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset\n# from utils import tokenizer, PAD, EOS\nfrom torch.nn.utils.rnn import pad_sequence\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:07:54.554030Z","iopub.execute_input":"2021-06-28T23:07:54.554356Z","iopub.status.idle":"2021-06-28T23:07:54.560959Z","shell.execute_reply.started":"2021-06-28T23:07:54.554322Z","shell.execute_reply":"2021-06-28T23:07:54.558349Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nPAD = tokenizer.pad_token_id\nEOS = tokenizer.convert_tokens_to_ids('.')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:07:54.562711Z","iopub.execute_input":"2021-06-28T23:07:54.563197Z","iopub.status.idle":"2021-06-28T23:07:57.976785Z","shell.execute_reply.started":"2021-06-28T23:07:54.563155Z","shell.execute_reply":"2021-06-28T23:07:57.975802Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e613a03f751f49aa9d3e24414a104990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a069265d59e4db4b25aee218eb939b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e227a9f31914821a74c108cf3210e86"}},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom pytorch_transformers import BertForSequenceClassification, modeling_bert\nfrom pytorch_transformers.modeling_bert import BertConfig\n\nbert_model = BertForSequenceClassification(BertConfig()).from_pretrained('bert-base-uncased')\n\nprint(bert_model)\n\nclass BertForClassification(modeling_bert.BertPreTrainedModel):\n    def __init__(self):\n        super(BertForClassification, self).__init__(BertConfig())\n\n        self.embeddings = bert_model.bert.embeddings\n        self.encoder = bert_model.bert.encoder\n        self.pooler = bert_model.bert.pooler\n        self.dropout = bert_model.dropout\n        self.classifier = bert_model.classifier\n        self.prediction = nn.Sequential( \n                nn.Linear(768, 64), \n                nn.ReLU(),\n                nn.Dropout(p=0.2),\n                nn.Linear(64, 1),\n        )\n\n        self.head_mask = [None] * 12\n\n        self.apply(self._init_weights)\n\n    def forward(self, text, position_ids, attention_mask):\n                      \n        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        attention_mask = (1.0 - attention_mask) * -10000.0   # ( 64, 1, 1, 71)\n\n        embeddings = self.embeddings(text)\n        hidden_states = self.encoder(embeddings, attention_mask, head_mask=self.head_mask)[0] # (64, 71, 768)\n        output = self.pooler(hidden_states) # (64, 768)\n        output = self.dropout(output) # (64, 768)\n        output = self.prediction(output) # (64, 2)\n        return output\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:07:57.980001Z","iopub.execute_input":"2021-06-28T23:07:57.980369Z","iopub.status.idle":"2021-06-28T23:08:21.540006Z","shell.execute_reply.started":"2021-06-28T23:07:57.980330Z","shell.execute_reply":"2021-06-28T23:08:21.539082Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 433/433 [00:00<00:00, 139691.84B/s]\n100%|██████████| 440473133/440473133 [00:14<00:00, 29968830.16B/s]\n","output_type":"stream"},{"name":"stdout","text":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_model(model, train_loss, val_loss, epoch, is_best):\n\n    state = {\n        'epoch' : epoch,\n        'train_loss' : train_loss,\n        'val_loss' : val_loss,\n        'model_state_dict' : model.state_dict()\n    }\n\n    file_name = './checkpoint_latest.pth.tar'\n    torch.save(state, file_name)\n\n    if is_best:\n        file_name = './checkpoint_best.pth.tar'\n        torch.save(state, file_name)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.541837Z","iopub.execute_input":"2021-06-28T23:08:21.542321Z","iopub.status.idle":"2021-06-28T23:08:21.549267Z","shell.execute_reply.started":"2021-06-28T23:08:21.542279Z","shell.execute_reply":"2021-06-28T23:08:21.548315Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CommonlitDataset(Dataset):\n    def __init__(self, root_dir, split):\n\n        self.root_dir = root_dir\n        self.split = split\n        \n        self.text_ids = []\n        self.text = []\n        self.targets = []\n        self.std_err = []\n        \n        if split == 'train':\n            self.load_data_train()\n            self.load_fn = self.get_item_train\n        else:\n            self.load_data_test()\n            self.load_fn = self.get_item_test\n    \n    def load_data_train(self):\n        \n        data_file = os.path.join(self.root_dir, self.split+'.csv')\n        df = pd.read_csv(data_file)\n        for i, row in df.iterrows():\n            self.text_ids.append(row['id'])\n            self.text.append(tokenizer.encode(row['excerpt']))\n            self.targets.append([row['target']])\n            self.std_err.append(row['standard_error'])\n    \n    def load_data_test(self):        \n        data_file = os.path.join(self.root_dir, self.split+'.csv')\n        df = pd.read_csv(data_file)\n        for i, row in df.iterrows():\n            self.text_ids.append(row['id'])\n            self.text.append(tokenizer.encode(row['excerpt']))\n   \n    def get_item_train(self, idx):\n        text_id = self.text_ids[idx]\n        text = self.text[idx]\n        text = text + [EOS]\n        text = torch.tensor(text, dtype=torch.long)\n\n        target = self.targets[idx]\n        std_err = self.std_err[idx]\n\n        return text_id, text, target, std_err\n\n    def get_item_test(self, idx):\n        text_id = self.text_ids[idx]\n        text = self.text[idx]\n        text = text + [EOS]\n        text = torch.tensor(text, dtype=torch.long)\n\n        return text_id, text\n    \n    def __len__(self):\n        return len(self.text_ids)\n\n    def __getitem__(self, idx):\n        return self.load_fn(idx)\n\ndef collate_fn_train(batch):\n\n    batch = list(zip(*batch))\n\n    text_id = batch[0]\n    text = pad_sequence(batch[1], batch_first=True, padding_value=PAD)\n    target = torch.tensor(batch[2], dtype=torch.float)\n    std_err = torch.tensor(batch[3], dtype=torch.float)\n\n    return text_id, text, target, std_err\n\n\ndef collate_fn_test(batch):\n\n    batch = list(zip(*batch))\n\n    text_id = batch[0]\n    text = pad_sequence(batch[1], batch_first=True, padding_value=PAD)\n\n    return text_id, text","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.550622Z","iopub.execute_input":"2021-06-28T23:08:21.551192Z","iopub.status.idle":"2021-06-28T23:08:21.573648Z","shell.execute_reply.started":"2021-06-28T23:08:21.551150Z","shell.execute_reply":"2021-06-28T23:08:21.572360Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n#from utils import PAD, save_model\nfrom torch.utils.data import DataLoader\n#from model import BertForClassification\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.sampler import SubsetRandomSampler\n#from dataset import CommonlitDataset, collate_fn_train, collate_fn_test\nfrom pytorch_transformers.optimization import AdamW, WarmupCosineSchedule","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.575707Z","iopub.execute_input":"2021-06-28T23:08:21.576608Z","iopub.status.idle":"2021-06-28T23:08:21.722860Z","shell.execute_reply.started":"2021-06-28T23:08:21.576567Z","shell.execute_reply":"2021-06-28T23:08:21.721878Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(model, train_data_loader, optimizer, scheduler, loss_fn, epoch, device):\n    \n    running_loss = 0.0\n    model.train()\n\n    with tqdm(desc='Epoch %d - Train' % epoch, unit='it', total=len(train_data_loader)) as pbar:\n\n        for idx, batch in enumerate(train_data_loader):\n            # text_id, text, target, std_err\n            text = batch[1].to(device)\n            target = batch[2].to(device)\n            std_err = batch[3].to(device)\n\n            optimizer.zero_grad()\n\n            position_ids = torch.arange(text.size(1), dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0).expand_as(text)\n\n            attention_mask = (text != PAD).float() #(batch_size, caption_len) (64, 21)\n            \n            pred = model(text, position_ids, attention_mask)\n            loss = loss_fn(pred, target)\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item()\n\n            used_mem = torch.cuda.max_memory_allocated() / 1024.0 ** 3\n\n            pbar.set_postfix(train_loss = running_loss /(idx + 1), mem = used_mem)\n            pbar.update()\n\n    return running_loss/len(train_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.727239Z","iopub.execute_input":"2021-06-28T23:08:21.727722Z","iopub.status.idle":"2021-06-28T23:08:21.744030Z","shell.execute_reply.started":"2021-06-28T23:08:21.727683Z","shell.execute_reply":"2021-06-28T23:08:21.743020Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def val(model, val_data_loader, loss_fn, epoch, device):\n    running_loss = 0.0\n    model.eval()\n\n    with tqdm(desc='Epoch %d - validation' % epoch, unit='it', total=len(val_data_loader)) as pbar:\n\n        for idx, batch in enumerate(train_data_loader):\n            # text_id, text, target, std_err\n            text = batch[1].to(device)\n            target = batch[2].to(device)\n            std_err = batch[3].to(device)\n\n            position_ids = torch.arange(text.size(1), dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0).expand_as(text)\n\n            attention_mask = (text != PAD).float() #(batch_size, caption_len) (64, 21)\n            \n            pred = model(text, position_ids, attention_mask)\n            loss = loss_fn(pred, target)\n            \n            running_loss += loss.item()\n\n            used_mem = torch.cuda.max_memory_allocated() / 1024.0 ** 3\n\n            pbar.set_postfix(validation_loss = running_loss /(idx + 1), mem = used_mem)\n            pbar.update()\n\n    return running_loss/len(val_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.749197Z","iopub.execute_input":"2021-06-28T23:08:21.751400Z","iopub.status.idle":"2021-06-28T23:08:21.763572Z","shell.execute_reply.started":"2021-06-28T23:08:21.751358Z","shell.execute_reply":"2021-06-28T23:08:21.762532Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def test(model, test_data_loader, epoch, device):\n    ids = []\n    target_values = []\n    model.eval()\n\n    with tqdm(desc='Epoch %d - test' % epoch, unit='it', total=len(test_data_loader)) as pbar:\n\n        for idx, batch in enumerate(test_data_loader):\n            # text_id, text, target, std_err\n            text_id = batch[0]\n            text = batch[1].to(device)\n\n            position_ids = torch.arange(text.size(1), dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0).expand_as(text)\n\n            attention_mask = (text != PAD).float() #(batch_size, caption_len) (64, 21)\n            \n            pred = model(text, position_ids, attention_mask)\n            \n            ids.append(text_id[0])\n            target_values.append(pred[0].cpu().item())\n    \n    df = pd.DataFrame({'id':ids,\n                        'target':target_values})\n    df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.769206Z","iopub.execute_input":"2021-06-28T23:08:21.771943Z","iopub.status.idle":"2021-06-28T23:08:21.783815Z","shell.execute_reply.started":"2021-06-28T23:08:21.771905Z","shell.execute_reply":"2021-06-28T23:08:21.782764Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nshuffle_dataset = True\nrandom_seed= 42\nnum_epochs=100\npatience=10\nmin_loss = np.inf\ndevice = \"cuda\"\n\nlr = 5e-5\nweight_decay = 1e-2\nbetas = (0.9, 0.999)\nwarmup_steps = 1000\nmax_steps = 100000","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.789177Z","iopub.execute_input":"2021-06-28T23:08:21.791375Z","iopub.status.idle":"2021-06-28T23:08:21.798776Z","shell.execute_reply.started":"2021-06-28T23:08:21.791338Z","shell.execute_reply":"2021-06-28T23:08:21.797866Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dataset = CommonlitDataset('../input/commonlitreadabilityprize', 'train')\n    \ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(0.2 * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:21.804231Z","iopub.execute_input":"2021-06-28T23:08:21.807243Z","iopub.status.idle":"2021-06-28T23:08:36.918721Z","shell.execute_reply.started":"2021-06-28T23:08:21.807204Z","shell.execute_reply":"2021-06-28T23:08:36.917963Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(dataset, \n                                    batch_size=batch_size,\n                                    drop_last=True,\n                                    collate_fn=collate_fn_train,\n                                    sampler=train_sampler,\n                                    num_workers=8)\n\nval_data_loader = DataLoader(dataset, \n                                batch_size=batch_size,\n                                drop_last=True,\n                                collate_fn=collate_fn_train,\n                                sampler=val_sampler,\n                                num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:36.919994Z","iopub.execute_input":"2021-06-28T23:08:36.920394Z","iopub.status.idle":"2021-06-28T23:08:36.925838Z","shell.execute_reply.started":"2021-06-28T23:08:36.920349Z","shell.execute_reply":"2021-06-28T23:08:36.925030Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test_dataset = CommonlitDataset('../input/commonlitreadabilityprize', 'test')\ntest_data_loader = DataLoader(test_dataset, \n                                batch_size=1,\n                                shuffle=True,\n                                drop_last=True,\n                                collate_fn=collate_fn_test,\n                                num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:36.927491Z","iopub.execute_input":"2021-06-28T23:08:36.928029Z","iopub.status.idle":"2021-06-28T23:08:36.982256Z","shell.execute_reply.started":"2021-06-28T23:08:36.927948Z","shell.execute_reply":"2021-06-28T23:08:36.981590Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = BertForClassification().to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:36.983958Z","iopub.execute_input":"2021-06-28T23:08:36.984215Z","iopub.status.idle":"2021-06-28T23:08:42.314064Z","shell.execute_reply.started":"2021-06-28T23:08:36.984191Z","shell.execute_reply":"2021-06-28T23:08:42.313264Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(\n        params=model.parameters(),\n        lr=lr,\n        weight_decay=weight_decay,\n        betas=betas\n    )\n\nscheduler = WarmupCosineSchedule(\n    optimizer=optimizer,\n    warmup_steps=warmup_steps,\n    t_total=max_steps\n)\n\nloss_fn = torch.nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:42.315419Z","iopub.execute_input":"2021-06-28T23:08:42.315761Z","iopub.status.idle":"2021-06-28T23:08:42.324037Z","shell.execute_reply.started":"2021-06-28T23:08:42.315726Z","shell.execute_reply":"2021-06-28T23:08:42.323084Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:42.325384Z","iopub.execute_input":"2021-06-28T23:08:42.325869Z","iopub.status.idle":"2021-06-28T23:08:42.343191Z","shell.execute_reply.started":"2021-06-28T23:08:42.325832Z","shell.execute_reply":"2021-06-28T23:08:42.342306Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"writer = SummaryWriter('./tensorboard_logs/')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:42.344728Z","iopub.execute_input":"2021-06-28T23:08:42.345281Z","iopub.status.idle":"2021-06-28T23:08:47.088138Z","shell.execute_reply.started":"2021-06-28T23:08:42.345243Z","shell.execute_reply":"2021-06-28T23:08:47.087289Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n\n    is_best_model = False\n\n    train_loss = train(model, train_data_loader, optimizer, scheduler, loss_fn, epoch, device)\n\n    val_loss = val(model, val_data_loader, loss_fn, epoch, device)\n\n    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n\n    if val_loss < min_loss:\n        is_best_model = True\n        min_loss = val_loss\n        test(model, test_data_loader, epoch, device)\n    else:\n        patience -= 1\n    \n    save_model(model, train_loss, val_loss, epoch, is_best_model)\n    \n    if patience <= 0:\n        break\nwriter.flush()\nwriter.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T23:08:47.091530Z","iopub.execute_input":"2021-06-28T23:08:47.091808Z","iopub.status.idle":"2021-06-28T23:10:42.768109Z","shell.execute_reply.started":"2021-06-28T23:08:47.091780Z","shell.execute_reply":"2021-06-28T23:10:42.766433Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Epoch 0 - Train:   0%|          | 0/141 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\nEpoch 0 - Train: 100%|██████████| 141/141 [01:12<00:00,  1.96it/s, mem=8.41, train_loss=1.38]\nEpoch 0 - validation: 141it [00:22,  6.34it/s, mem=12.3, validation_loss=1.09]                        \nEpoch 0 - test:   0%|          | 0/7 [00:00<?, ?it/s]\nEpoch 1 - Train:  25%|██▍       | 35/141 [00:18<00:55,  1.92it/s, mem=12.3, train_loss=1.09]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-e4a81adee69c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mis_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-7ac2bcd7926c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, optimizer, scheduler, loss_fn, epoch, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}